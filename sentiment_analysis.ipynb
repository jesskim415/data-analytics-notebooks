{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Created on Tue Apr  2 17:03:53 2019\n",
    "\n",
    "@author: jesskim\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer # TfidfVectorizer\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize.api import TokenizerI\n",
    "from nltk.tokenize import PunktSentenceTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Positive    23998\n",
       "Negative     8271\n",
       "Neutral      5158\n",
       "Name: Sentiment, dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv('googleplaystore_user_reviews.csv')\n",
    "dataset = dataset.dropna()\n",
    "dataset['Sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jesskim/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "/Users/jesskim/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "dataset.Sentiment[dataset.Sentiment =='Positive'] = 0\n",
    "dataset.Sentiment[dataset.Sentiment =='Neutral'] = 1\n",
    "dataset.Sentiment[dataset.Sentiment =='Negative'] = 2\n",
    "\n",
    "dataset['Sentiment'] = dataset['Sentiment'].astype('int')\n",
    "#dataset[\"index\"] = range(0,37427)\n",
    "dataset['index'] = range(dataset.shape[0])\n",
    "dataset = dataset.set_index(\"index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort by most positive reviews\n",
    "ranked_app = dataset.groupby('App').mean()['Sentiment_Polarity'].sort_values(ascending=False)\n",
    "##ranked_app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer() \n",
    "\n",
    "def clean_text(review):\n",
    "    review = re.sub('[^a-zA-Z]', ' ', review) \n",
    "    review = re.sub('[/(){}\\[\\]\\|@!,;]', ' ', review)\n",
    "    review = re.sub('[^0-9a-z #+_♥️]', ' ', review) #Remove bad symbols\n",
    "    review = review.lower()\n",
    "    review = review.split()\n",
    "   \n",
    "    review = [stemmer.stem(token) for token in review if token not in set(stopwords.words('english'))]\n",
    "    review =' '.join(review)\n",
    "    \n",
    "    return review\n",
    "\n",
    "corpus = dataset.Translated_Review.apply(clean_text).tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Extraction Method 1: TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cv = TfidfVectorizer(max_features = 16000)\n",
    "cv = TfidfVectorizer(max_features = 32000, max_df=0.8, min_df=0.0001, ngram_range=[1, 2]) \n",
    "X_tfidf = cv.fit_transform(corpus).toarray()\n",
    "y_tfidf = dataset.iloc[:, 2].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Extraction Method 2: Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Download Doc2Vec model from https://ibm.ent.box.com/s/3f160t4xpuya9an935k84ig465gvymm2\n",
    "\"\"\"\n",
    "\n",
    "model_path = '/Users/jesskim/Downloads/enwiki_dbow/doc2vec.bin'\n",
    "\n",
    "tokenized_corpus = [word_tokenize(doc) for doc in corpus]\n",
    "dv_model = Doc2Vec.load(model_path)\n",
    "\n",
    "X_dv = np.array([dv_model.infer_vector(tokenized_doc) for tokenized_doc in tokenized_corpus])\n",
    "y_dv = dataset.iloc[:, 2].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(mode):\n",
    "    if mode == 'tfidf':\n",
    "        return X_tfidf, y_tfidf\n",
    "    elif mode == 'doc2vec':\n",
    "        return X_dv, y_dv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CHANGE this to change method for feature extraction\n",
    "X, y = extract_features('doc2vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "CLASSIFIERS = {\n",
    "    'gaussian': GaussianNB(),\n",
    "    'rf': RandomForestClassifier(n_estimators = 10, criterion = 'entropy', random_state = 0),\n",
    "    'dt': DecisionTreeClassifier(criterion = 'entropy', random_state = 0),\n",
    "    'svc': SVC(kernel = 'linear', random_state = 0)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = CLASSIFIERS['dt']\n",
    "classifier.fit(X_train, y_train)\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.trace(cm)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
